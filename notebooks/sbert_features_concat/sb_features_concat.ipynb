{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPNj8oSOZaSUXnY4jQDzNLV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#해당 파일에서 test_paragraph.csv 생성"],"metadata":{"id":"EscLFqy7BHUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z61YALsT-A15","executionInfo":{"status":"ok","timestamp":1751597057456,"user_tz":-540,"elapsed":16818,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"db6b31e1-33d8-4b4a-9982-b6a3c1f5daae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","\n","\n","# 베이스 경로 설정\n","BASE_DIR = '/content/drive/MyDrive/Dacon_FakeText/'"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n"],"metadata":{"id":"76DsnxDw-Mvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트셋 CSV 로드\n","test_df = pd.read_csv(BASE_DIR + 'data/test.csv')\n","\n","# Test 데이터에 Feature Engineering 적용\n","test_df['char_count'] = test_df['paragraph_text'].apply(len)\n","test_df['word_count'] = test_df['paragraph_text'].apply(lambda x: len(x.split()))\n","test_df['sentence_count'] = test_df['paragraph_text'].apply(lambda x: x.count('.') + x.count('!') + x.count('?'))\n","\n","# 문단 위치 피처 추가 (title별 그룹화)\n","def add_position_features(df):\n","    position_info = []\n","    for title, group in df.groupby('title'):\n","        total_paragraphs = group.shape[0]\n","        for idx in group['paragraph_index']:\n","            rel_pos = idx / (total_paragraphs - 1) if total_paragraphs > 1 else 0\n","            position_info.append((idx, rel_pos))\n","    abs_idx, rel_idx = zip(*position_info)\n","    df['absolute_position'] = abs_idx\n","    df['relative_position'] = rel_idx\n","    return df\n","\n","test_df = add_position_features(test_df)\n","\n","# 최종 저장\n","test_df.to_csv(BASE_DIR + 'data/test_paragraph.csv', index=False, encoding='utf-8')\n","\n","print(\"test_paragraph.csv 저장 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJK8R_7fAn2X","executionInfo":{"status":"ok","timestamp":1751597060232,"user_tz":-540,"elapsed":1617,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"24862a1d-5901-48e1-e594-96cf63386abe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ test_paragraph.csv 저장 완료!\n"]}]},{"cell_type":"code","source":["# 기존 문단 데이터프레임 로드\n","train_paragraph_df = pd.read_csv(BASE_DIR + 'data/train_paragraph.csv')\n","test_paragraph_df = pd.read_csv(BASE_DIR + 'data/test_paragraph.csv')\n","\n","# 문단 길이 피처\n","train_paragraph_df['char_count'] = train_paragraph_df['paragraph_text'].apply(len)\n","test_paragraph_df['char_count'] = test_paragraph_df['paragraph_text'].apply(len)\n","\n","# 단어 수 피처\n","train_paragraph_df['word_count'] = train_paragraph_df['paragraph_text'].apply(lambda x: len(x.split()))\n","test_paragraph_df['word_count'] = test_paragraph_df['paragraph_text'].apply(lambda x: len(x.split()))\n","\n","# 문장 수 피처 (간단히 마침표, 느낌표, 물음표로 분리)\n","def count_sentences(text):\n","    return text.count('.') + text.count('!') + text.count('?')\n","\n","train_paragraph_df['sentence_count'] = train_paragraph_df['paragraph_text'].apply(count_sentences)\n","test_paragraph_df['sentence_count'] = test_paragraph_df['paragraph_text'].apply(count_sentences)\n","\n","# 문단 위치 (절대 index / 상대 비율)\n","def add_position_features(df):\n","    position_info = []\n","    for title, group in df.groupby('title'):\n","        total_paragraphs = group.shape[0]\n","        for idx in group['paragraph_index']:\n","            rel_pos = idx / (total_paragraphs - 1) if total_paragraphs > 1 else 0\n","            position_info.append((idx, rel_pos))\n","    abs_idx, rel_idx = zip(*position_info)\n","    df['absolute_position'] = abs_idx\n","    df['relative_position'] = rel_idx\n","    return df\n","\n","train_paragraph_df = add_position_features(train_paragraph_df)\n","test_paragraph_df = add_position_features(test_paragraph_df)\n","\n","# 최종 저장\n","train_paragraph_df.to_csv(BASE_DIR + 'data/train_paragraph_with_features.csv', index=False, encoding='utf-8')\n","test_paragraph_df.to_csv(BASE_DIR + 'data/test_paragraph_with_features.csv', index=False, encoding='utf-8')\n","\n","print(\"버전1 Feature Engineering 저장 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a63lWvBn-Mtp","executionInfo":{"status":"ok","timestamp":1751597113283,"user_tz":-540,"elapsed":53046,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"afab1118-1a0b-41c8-a8cf-20bece40b568"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ 버전1 Feature Engineering 저장 완료!\n"]}]},{"cell_type":"code","source":["# TF-IDF Vectorizer 학습 (train + test 전체로)\n","all_texts = pd.concat([train_paragraph_df['paragraph_text'], test_paragraph_df['paragraph_text']]).tolist()\n","vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n","vectorizer.fit(all_texts)\n","\n","# TF-IDF 통계 피처 추가 함수\n","def get_tfidf_stats(texts):\n","    tfidf_matrix = vectorizer.transform(texts)\n","    mean_tfidf = tfidf_matrix.mean(axis=1)\n","    max_tfidf = tfidf_matrix.max(axis=1).toarray().flatten()\n","    return np.array(mean_tfidf).flatten(), max_tfidf\n","\n","# Train\n","mean_tfidf_train, max_tfidf_train = get_tfidf_stats(train_paragraph_df['paragraph_text'].tolist())\n","train_paragraph_df['tfidf_mean'] = mean_tfidf_train\n","train_paragraph_df['tfidf_max'] = max_tfidf_train\n","\n","# Test\n","mean_tfidf_test, max_tfidf_test = get_tfidf_stats(test_paragraph_df['paragraph_text'].tolist())\n","test_paragraph_df['tfidf_mean'] = mean_tfidf_test\n","test_paragraph_df['tfidf_max'] = max_tfidf_test\n","\n","# 최종 저장\n","train_paragraph_df.to_csv(BASE_DIR + 'data/train_paragraph_with_features_v2.csv', index=False, encoding='utf-8')\n","test_paragraph_df.to_csv(BASE_DIR + 'data/test_paragraph_with_features_v2.csv', index=False, encoding='utf-8')\n","\n","print(\"버전2 Feature Engineering (TF-IDF 포함) 저장 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"LrddiMjJ-NXW","executionInfo":{"status":"error","timestamp":1751597411511,"user_tz":-540,"elapsed":298220,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"83824c04-84d6-43fd-beb2-71455b1cd5bc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-9-1919785172.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_paragraph_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_paragraph_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ✅ TF-IDF 통계 피처 추가 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2072\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         )\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_df corresponds to < documents than min_df\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m             X = self._limit_features(\n\u001b[1;32m   1390\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreordered\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \"\"\"\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0msorted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0mmap_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# SBERT 임베딩 불러오기\n","X_sbert_train = np.load(BASE_DIR + 'data/embeddings/train_sbert_embeddings.npy')\n","X_sbert_test = np.load(BASE_DIR + 'data/embeddings/test_sbert_embeddings.npy')\n","\n","# 버전1과 버전2 CSV 불러오기\n","v1_train_df = pd.read_csv(BASE_DIR + 'data/train_paragraph_with_features.csv')\n","v2_train_df = pd.read_csv(BASE_DIR + 'data/train_paragraph_with_features_v2.csv')\n","\n","v1_test_df = pd.read_csv(BASE_DIR + 'data/test_paragraph_with_features.csv')\n","v2_test_df = pd.read_csv(BASE_DIR + 'data/test_paragraph_with_features_v2.csv')\n","\n","# 버전1과 버전2 CSV 병합 (id 또는 index 맞게 조인)\n","train_df = pd.concat([v1_train_df, v2_train_df[['tfidf_mean', 'tfidf_max']]], axis=1)\n","test_df = pd.concat([v1_test_df, v2_test_df[['tfidf_mean', 'tfidf_max']]], axis=1)\n","\n","# tf-idf + 특징 feature 컬럼 정의\n","feature_cols = [\n","    'char_count',\n","    'word_count',\n","    'sentence_count',\n","    'absolute_position',\n","    'relative_position',\n","    'tfidf_mean',\n","    'tfidf_max'\n","]\n","\n","# numpy 변환\n","X_feat_train = train_df[feature_cols].values\n","X_feat_test = test_df[feature_cols].values\n","\n","\n","\n","\n","\n","####기존버전과 다름 사용시 고쳐야함\n","# 문단 간 관계 피처 계산\n","avg_train, max_train, cos_mean_train, cos_max_train = get_title_agg_sbert_features(X_sbert_train, train_df)\n","avg_test, max_test, cos_mean_test, cos_max_test = get_title_agg_sbert_features(X_sbert_test, test_df)\n","\n","# 최종 Concat: [SBERT, title 평균, title 최대, TF-IDF 등 특징 피처, cosine mean/max]\n","X_train = np.concatenate([X_sbert_train, avg_train, max_train, X_feat_train, cos_mean_train, cos_max_train], axis=1)\n","X_test = np.concatenate([X_sbert_test, avg_test, max_test, X_feat_test, cos_mean_test, cos_max_test], axis=1)\n","\n","print(f\"✅ 문단 간 관계 피처 포함 최종 X_train shape: {X_train.shape}\")\n","print(f\"✅ 문단 간 관계 피처 포함 최종 X_test shape: {X_test.shape}\")\n","\n","# 저장\n","output_dir = BASE_DIR + 'data/embeddings/'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","np.save(output_dir + 'train_concat_1.npy', X_train)\n","np.save(output_dir + 'test_concat_1.npy', X_test)\n","\n","print(\"최종 3피처 concat 버전 저장 완료!\")"],"metadata":{"id":"JcEMaRcF-jGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oR4ZoeYuGNu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YlegarKdGOPn"},"execution_count":null,"outputs":[]}]}