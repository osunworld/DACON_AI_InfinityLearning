{"cells":[{"cell_type":"code","source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from transformers import (\n","    AutoTokenizer, AutoModel,\n","    TrainingArguments, Trainer,\n","    EarlyStoppingCallback\n",")\n","from sklearn.model_selection import GroupKFold\n","from google.colab import drive\n","from safetensors.torch import load_file\n","\n"],"metadata":{"id":"D3fNsIeQy_Cv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1136,"status":"ok","timestamp":1752511048191,"user":{"displayName":"Ï†ïÌÉúÏñë","userId":"07065479975462546638"},"user_tz":-540},"id":"yzgnW81Z3RBT","outputId":"6938c0e5-f673-480f-9c5c-70a676b23eaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Google Drive ÎßàÏö¥Ìä∏ Ï§ë...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["print(\"Google Drive ÎßàÏö¥Ìä∏ Ï§ë...\")\n","drive.mount('/content/drive')\n","\n","BASE_DIR = '/content/drive/MyDrive/Dacon_FakeText'\n","train_csv_path = f\"{BASE_DIR}/data/train.csv\"\n","test_csv_path = f\"{BASE_DIR}/data/test.csv\"\n","model_save_path = f\"{BASE_DIR}/data/my_best_model_final\"\n","submission_path = f\"{BASE_DIR}/data/submission_final.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHNnYaX72vtn"},"outputs":[],"source":["EPOCHS = 4\n","BATCH_SIZE = 100\n","LEARNING_RATE = 2e-5\n","N_SPLITS = 3\n","\n","\n","def preprocess_train_csv(train_csv_path):\n","    df = pd.read_csv(train_csv_path)\n","    processed = []\n","    for _, row in df.iterrows():\n","        title = row[\"title\"]\n","        full_text = row[\"full_text\"]\n","        label = row[\"generated\"]\n","        paragraphs = full_text.split(\"\\n\\n\")\n","        for idx, para in enumerate(paragraphs):\n","            para = para.strip()\n","            if len(para) > 10:\n","                processed.append({\n","                    \"title\": title,\n","                    \"paragraph_index\": idx,\n","                    \"paragraph_text\": para,\n","                    \"label\": label\n","                })\n","    return pd.DataFrame(processed)\n","\n","class ParagraphDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length=512):\n","        self.texts = df[\"paragraph_text\"].tolist()\n","        self.labels = df[\"label\"].tolist()\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        inputs = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        item = {key: val.squeeze(0) for key, val in inputs.items() if key != 'token_type_ids'}\n","        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n","        return item\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlynpOCk31BH"},"outputs":[],"source":["class RobertaRegressionModel(nn.Module):\n","    def __init__(self, model_name=\"klue/roberta-base\"):\n","        super().__init__()\n","        self.bert = AutoModel.from_pretrained(model_name)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.hidden = nn.Linear(self.bert.config.hidden_size, 256)\n","        self.act = nn.ReLU()\n","        self.out = nn.Linear(256, 1)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        cls_output = self.dropout(outputs.last_hidden_state[:, 0, :])\n","        hidden_out = self.act(self.hidden(cls_output))\n","        logits = self.out(hidden_out).squeeze()\n","\n","        loss = None\n","        if labels is not None:\n","            loss = nn.BCEWithLogitsLoss()(logits, labels)\n","        return (loss, logits) if loss is not None else logits"]},{"cell_type":"code","source":["def train_model_with_folds(train_csv_path, model_save_dir, n_splits=3):\n","    df = preprocess_train_csv(train_csv_path)\n","    tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n","    group_kfold = GroupKFold(n_splits=n_splits)\n","    groups = df[\"title\"]\n","\n","    for fold, (train_idx, val_idx) in enumerate(group_kfold.split(df, df[\"label\"], groups)):\n","        print(f\"\\nüöÄ Fold {fold} ÏãúÏûë\")\n","        train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n","\n","        train_dataset = ParagraphDataset(train_df, tokenizer)\n","        val_dataset = ParagraphDataset(val_df, tokenizer)\n","\n","        model = RobertaRegressionModel()\n","\n","        training_args = TrainingArguments(\n","            output_dir=f\"./results/fold{fold}\",\n","            per_device_train_batch_size=BATCH_SIZE,\n","            per_device_eval_batch_size=BATCH_SIZE,\n","            num_train_epochs=EPOCHS,\n","            learning_rate=LEARNING_RATE,\n","            logging_dir=f\"./logs/fold{fold}\",\n","            logging_steps=100,\n","            save_total_limit=1,\n","            report_to=\"none\"\n","        )\n","\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            tokenizer=tokenizer\n","        )\n","\n","        trainer.train()\n","\n","        fold_dir = os.path.join(model_save_dir, f\"fold{fold}\")\n","        os.makedirs(fold_dir, exist_ok=True)\n","        trainer.save_model(fold_dir)\n","        tokenizer.save_pretrained(fold_dir)\n","        print(f\" Fold {fold} Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å\")"],"metadata":{"id":"x2GobfGv3kwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def predict_with_ensemble(test_csv_path, model_save_dir, submission_path, n_splits=3):\n","    test_df = pd.read_csv(test_csv_path)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    preds_all = []\n","\n","    for fold in range(n_splits):\n","        print(f\"üîç Fold {fold} Î™®Îç∏Î°ú Ï∂îÎ°† Ï§ë...\")\n","        fold_dir = os.path.join(model_save_dir, f\"fold{fold}\")\n","        tokenizer = AutoTokenizer.from_pretrained(fold_dir)\n","        model = RobertaRegressionModel().to(device)\n","        model.load_state_dict(load_file(os.path.join(fold_dir, \"model.safetensors\")))\n","        model.eval()\n","\n","        preds = []\n","        for text in test_df[\"paragraph_text\"]:\n","            inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n","            inputs = {k: v.to(device) for k, v in inputs.items() if k != 'token_type_ids'}\n","            output = model(**inputs)\n","            logit = output if not isinstance(output, tuple) else output[1]\n","            prob = torch.sigmoid(logit).item()\n","            preds.append(prob)\n","\n","        preds_all.append(preds)\n","\n","    # ÌèâÍ∑†\n","    final_preds = torch.tensor(preds_all).mean(dim=0).tolist()\n","    test_df[\"generated\"] = final_preds\n","    test_df[[\"ID\", \"generated\"]].to_csv(submission_path, index=False)\n","    print(\"Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å!\")\n"],"metadata":{"id":"0WIdDYY13kr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ÌïôÏäµ\n","train_model_with_folds(train_csv_path, model_save_dir=\"./final_model\", n_splits=3)\n","\n","# Ï∂îÎ°†\n","predict_with_ensemble(test_csv_path, model_save_dir=\"./final_model\", submission_path=submission_path, n_splits=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6GhWJpoU3klY","executionInfo":{"status":"error","timestamp":1752528677550,"user_tz":-540,"elapsed":16970864,"user":{"displayName":"Ï†ïÌÉúÏñë","userId":"07065479975462546638"}},"outputId":"1e3b68a3-10ee-46bf-f4dc-366b691f4489"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","üöÄ Fold 0 ÏãúÏûë\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-26-2833542294.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2592' max='2592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2592/2592 1:34:55, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.209400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.156000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.146800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.147700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.129000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.118500</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.103000</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.101400</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.090800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.085200</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.090500</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.097700</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.080400</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.054800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.080400</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.063700</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.062900</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.059800</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.061100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.054600</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.040500</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.052000</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.044600</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.047700</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.051800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Fold 0 Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å\n","\n","üöÄ Fold 1 ÏãúÏûë\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipython-input-26-2833542294.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2592' max='2592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2592/2592 1:34:56, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.217100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.145100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.141600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.129000</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.142900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.129200</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.125600</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.103900</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.099400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.094100</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.087000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.088200</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.086000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.068100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.062400</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.058900</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.061000</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.058400</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.062100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.049300</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.046800</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.045700</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.041900</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.040900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.037400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Fold 1 Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å\n","\n","üöÄ Fold 2 ÏãúÏûë\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipython-input-26-2833542294.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2592' max='2592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2592/2592 1:32:26, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.200200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.145800</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.139200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.132300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.127500</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.120500</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.110100</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.084500</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.087900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.084900</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.093200</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.073100</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.078600</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.058100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.059200</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.062900</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.050500</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.051900</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.052400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.043900</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.042400</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.031600</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.044800</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.041300</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.032900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Fold 2 Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å\n","üîç Fold 0 Î™®Îç∏Î°ú Ï∂îÎ°† Ï§ë...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './final_model/fold0/pytorch_model.bin'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-28-2360649631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Ï∂îÎ°†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredict_with_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./final_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubmission_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-27-1364878646.py\u001b[0m in \u001b[0;36mpredict_with_ensemble\u001b[0;34m(test_csv_path, model_save_dir, submission_path, n_splits)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pytorch_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './final_model/fold0/pytorch_model.bin'"]}]},{"cell_type":"code","source":["predict_with_ensemble(test_csv_path, model_save_dir=\"./final_model\", submission_path=submission_path, n_splits=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yu5HT10XORgC","executionInfo":{"status":"ok","timestamp":1752535607963,"user_tz":-540,"elapsed":61791,"user":{"displayName":"Ï†ïÌÉúÏñë","userId":"07065479975462546638"}},"outputId":"80c25bc1-abcd-4764-d1ab-15948b1bbb05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["üîç Fold 0 Î™®Îç∏Î°ú Ï∂îÎ°† Ï§ë...\n","üîç Fold 1 Î™®Îç∏Î°ú Ï∂îÎ°† Ï§ë...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["üîç Fold 2 Î™®Îç∏Î°ú Ï∂îÎ°† Ï§ë...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMaQTgzI35rA"},"outputs":[],"source":["# # 7. ÌååÏù¥ÌîÑÎùºÏù∏ Ìï®Ïàò\n","\n","# def run_pipeline(train_csv_path, test_csv_path, model_save_path, submission_path):\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n","\n","#     train_df = preprocess_train_csv(train_csv_path)\n","#     train_dataset = ParagraphDataset(train_df, tokenizer)\n","\n","#     model = RobertaRegressionModel().to(device)\n","\n","#     training_args = TrainingArguments(\n","#         output_dir=\"./results\",\n","#         per_device_train_batch_size=BATCH_SIZE,\n","#         num_train_epochs=EPOCHS,\n","#         learning_rate=LEARNING_RATE,\n","#         save_total_limit=1,\n","#         save_strategy=\"no\",\n","#         report_to=\"none\",\n","#         logging_dir=\"./logs\",\n","#         logging_steps=100,\n","#     )\n","\n","#     trainer = Trainer(\n","#         model=model,\n","#         args=training_args,\n","#         train_dataset=train_dataset,\n","#         tokenizer=tokenizer\n","#     )\n","\n","#     trainer.train()\n","#     model.eval()\n","\n","#     # Î™®Îç∏ Ï†ÄÏû•\n","#     os.makedirs(model_save_path, exist_ok=True)\n","#     torch.save(model.state_dict(), os.path.join(model_save_path, \"pytorch_model.bin\"))\n","#     tokenizer.save_pretrained(model_save_path)\n","\n","#     # ÏòàÏ∏°\n","#     test_df = pd.read_csv(test_csv_path)\n","#     preds = []\n","#     for text in test_df[\"paragraph_text\"]:\n","#         inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n","#         inputs = {k: v.to(device) for k, v in inputs.items()}\n","#         with torch.no_grad():\n","#             output = model(**inputs)\n","#             pred = output[1].item() if isinstance(output, tuple) else output.item()\n","#             preds.append(pred)\n","\n","#     # Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n","#     test_df[\"generated\"] = preds\n","#     test_df[[\"ID\", \"generated\"]].to_csv(submission_path, index=False)\n","#     print(\"Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å!\")\n","\n","# # 8. Ïã§Ìñâ\n","# run_pipeline(train_csv_path, test_csv_path, model_save_path, submission_path)\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"M5T-cAd82i9L"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}