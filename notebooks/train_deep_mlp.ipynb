{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPMaQzM3olDC9TGGeuYNsgb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"91bZ1iB6qffl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751547865842,"user_tz":-540,"elapsed":24269,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"ccaa6d67-f1c7-4a43-9b1f-66c52bee8d67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","\n","\n","# 베이스 경로 설정\n","BASE_DIR = '/content/drive/MyDrive/Dacon_FakeText/'\n","SAVE_PATH = BASE_DIR + 'data/embeddings/train_concat.npy'"]},{"cell_type":"code","source":["!pip install iterative-stratification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDOVdsf1Y5k2","executionInfo":{"status":"ok","timestamp":1751547910724,"user_tz":-540,"elapsed":6487,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"1ee41301-6cbf-459b-bf19-28844915cd28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting iterative-stratification\n","  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.15.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.6.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n","Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.9\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from tqdm import tqdm\n","import os"],"metadata":{"id":"SqkF_FooqqsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. 설정값\n","n_splits = 5\n","batch_size = 512\n","epochs = 30\n","early_stopping_rounds = 4\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"bDdnuWkqqzda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. 데이터 로딩\n","X_train = np.load(BASE_DIR + 'data/embeddings/train_concat.npy')\n","train_meta = pd.read_csv(BASE_DIR + 'data/train_paragraph.csv')\n","y_train = train_meta['generated'].values\n","groups = train_meta['title'].values\n","\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cCQGJr_q58B","executionInfo":{"status":"ok","timestamp":1751548034405,"user_tz":-540,"elapsed":118098,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"ec5d213f-b727-469b-8a87-dcd77db5c202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (1226364, 775), y_train shape: (1226364,)\n"]}]},{"cell_type":"code","source":["# 3. PyTorch Dataset 클래스\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.y = torch.tensor(y, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n"],"metadata":{"id":"qN7WlfmKrLoN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Focal Loss with Logits 정의\n","class FocalLossWithLogits(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n","        super(FocalLossWithLogits, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, logits, targets):\n","        # targets: 0~1 float (binary labels)\n","        targets = targets.view(-1, 1)  # (batch_size, 1)\n","\n","        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')  # 이미 내부적으로 sigmoid 포함됨\n","        pred_prob = torch.sigmoid(logits)  # pt 확률값 구하기\n","        pt = pred_prob * targets + (1 - pred_prob) * (1 - targets)\n","        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        else:\n","            return focal_loss.sum()"],"metadata":{"id":"QjhjYLn9Ay9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class StratifiedGroupKFold:\n","    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n","        self.n_splits = n_splits\n","        self.shuffle = shuffle\n","        self.random_state = random_state\n","\n","    def split(self, X, y, groups):\n","        # 그룹을 정수로 인코딩\n","        if not np.issubdtype(groups.dtype, np.number):\n","            groups = LabelEncoder().fit_transform(groups)\n","\n","        # 그룹별 인덱스 저장\n","        group_to_indices = {}\n","        for idx, g in enumerate(groups):\n","            group_to_indices.setdefault(g, []).append(idx)\n","\n","        unique_groups = np.array(list(group_to_indices.keys()))\n","        group_y = np.array([\n","            int(y[group_to_indices[g]].mean() >= 0.5) for g in unique_groups\n","        ])\n","\n","        skf = StratifiedKFold(\n","            n_splits=self.n_splits,\n","            shuffle=self.shuffle,\n","            random_state=self.random_state\n","        )\n","\n","        for group_train_idx, group_val_idx in skf.split(unique_groups, group_y):\n","            train_indices, val_indices = [], []\n","\n","            for gi in group_train_idx:\n","                train_indices.extend(group_to_indices[unique_groups[gi]])\n","            for gi in group_val_idx:\n","                val_indices.extend(group_to_indices[unique_groups[gi]])\n","\n","            yield np.array(train_indices), np.array(val_indices)"],"metadata":{"id":"JlkEQMEvSOAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. MLP 모델 클래스\n","class MLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(MLP, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(64,1)\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"PxbUe4CCspqb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='max',           # val_auc가 증가해야 하므로 'max'\n","    factor=0.5,           # lr을 절반으로 줄임\n","    patience=3,           # 2 epoch 동안 개선 없으면 감소\n","    verbose=True,\n","    min_lr=1e-5           # 최소 학습률 하한\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Pr7H_wlu-0V","executionInfo":{"status":"ok","timestamp":1751554681885,"user_tz":-540,"elapsed":32,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"ac17a5b2-ce18-4b9c-cdec-583c5de9e5b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 5. GroupKFold + 학습 + 로그 저장\n","sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n","oof_preds = np.zeros(len(X_train))\n","\n","all_logs = []  # 전체 로그 저장용 리스트\n","y_train=y_train.astype(np.float32)\n","for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train, y_train, groups)):\n","    print(f'\\n=== Fold {fold+1}/{n_splits} 학습 시작 ===')\n","\n","    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n","    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n","\n","    train_dataset = CustomDataset(X_tr, y_tr)\n","    val_dataset = CustomDataset(X_val, y_val)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    model = MLP(input_dim=X_train.shape[1]).to(device)\n","    criterion = FocalLossWithLogits(alpha=0.25, gamma=2.0)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n","\n","    best_auc = 0\n","    patience = 0\n","    fold_log = []  # 이 fold의 epoch별 로그 저장용 리스트\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            optimizer.zero_grad()\n","            preds = model(xb)\n","            loss = criterion(preds, yb.view(-1,1))\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Validation\n","        model.eval()\n","        val_preds = []\n","        with torch.no_grad():\n","            for xb, _ in val_loader:\n","                xb = xb.to(device)\n","                pred = model(xb).squeeze()\n","                pred = torch.sigmoid(pred)\n","                val_preds.extend(pred.cpu().numpy())\n","\n","        auc = roc_auc_score(y_val, val_preds)\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        print(f\"Fold {fold+1} | Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val AUC: {auc:.4f}\")\n","\n","        # LR Scheduler 업데이트\n","        scheduler.step(auc)\n","\n","        # 로그 저장\n","        fold_log.append({\n","            'fold': fold + 1,\n","            'epoch': epoch + 1,\n","            'train_loss': avg_train_loss,\n","            'val_auc': auc\n","        })\n","\n","        # Early Stopping\n","        if auc > best_auc:\n","            best_auc = auc\n","            patience = 0\n","            torch.save(model.state_dict(), BASE_DIR + f'model/mlp_fold{fold+1}.pt')\n","            print(f\"Fold {fold+1} 모델 저장 (Best AUC: {best_auc:.4f})\")\n","        else:\n","            patience += 1\n","            if patience >= early_stopping_rounds:\n","                print(f\"Early Stopping (patience {early_stopping_rounds} 도달)\")\n","                break\n","\n","    # Fold별 로그 저장\n","    all_logs.extend(fold_log)\n","\n","    # Fold OOF\n","    oof_preds[val_idx] = val_preds\n","\n"],"metadata":{"id":"A30Y75EXrLlE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751557362262,"user_tz":-540,"elapsed":2679112,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"4b75f394-e5fa-4d9d-e8d1-939017352f85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Fold 1/5 학습 시작 ===\n","Fold 1 | Epoch 1 | Train Loss: 0.0187 | Val AUC: 0.6951\n","✅ Fold 1 모델 저장 (Best AUC: 0.6951)\n","Fold 1 | Epoch 2 | Train Loss: 0.0163 | Val AUC: 0.7010\n","✅ Fold 1 모델 저장 (Best AUC: 0.7010)\n","Fold 1 | Epoch 3 | Train Loss: 0.0159 | Val AUC: 0.7099\n","✅ Fold 1 모델 저장 (Best AUC: 0.7099)\n","Fold 1 | Epoch 4 | Train Loss: 0.0157 | Val AUC: 0.7137\n","✅ Fold 1 모델 저장 (Best AUC: 0.7137)\n","Fold 1 | Epoch 5 | Train Loss: 0.0156 | Val AUC: 0.7143\n","✅ Fold 1 모델 저장 (Best AUC: 0.7143)\n","Fold 1 | Epoch 6 | Train Loss: 0.0156 | Val AUC: 0.7166\n","✅ Fold 1 모델 저장 (Best AUC: 0.7166)\n","Fold 1 | Epoch 7 | Train Loss: 0.0156 | Val AUC: 0.7201\n","✅ Fold 1 모델 저장 (Best AUC: 0.7201)\n","Fold 1 | Epoch 8 | Train Loss: 0.0155 | Val AUC: 0.7236\n","✅ Fold 1 모델 저장 (Best AUC: 0.7236)\n","Fold 1 | Epoch 9 | Train Loss: 0.0155 | Val AUC: 0.7217\n","Fold 1 | Epoch 10 | Train Loss: 0.0154 | Val AUC: 0.7238\n","✅ Fold 1 모델 저장 (Best AUC: 0.7238)\n","Fold 1 | Epoch 11 | Train Loss: 0.0154 | Val AUC: 0.7250\n","✅ Fold 1 모델 저장 (Best AUC: 0.7250)\n","Fold 1 | Epoch 12 | Train Loss: 0.0154 | Val AUC: 0.7272\n","✅ Fold 1 모델 저장 (Best AUC: 0.7272)\n","Fold 1 | Epoch 13 | Train Loss: 0.0154 | Val AUC: 0.7259\n","Fold 1 | Epoch 14 | Train Loss: 0.0154 | Val AUC: 0.7288\n","✅ Fold 1 모델 저장 (Best AUC: 0.7288)\n","Fold 1 | Epoch 15 | Train Loss: 0.0153 | Val AUC: 0.7255\n","Fold 1 | Epoch 16 | Train Loss: 0.0153 | Val AUC: 0.7288\n","Fold 1 | Epoch 17 | Train Loss: 0.0153 | Val AUC: 0.7295\n","✅ Fold 1 모델 저장 (Best AUC: 0.7295)\n","Fold 1 | Epoch 18 | Train Loss: 0.0153 | Val AUC: 0.7277\n","Fold 1 | Epoch 19 | Train Loss: 0.0153 | Val AUC: 0.7293\n","Fold 1 | Epoch 20 | Train Loss: 0.0153 | Val AUC: 0.7319\n","✅ Fold 1 모델 저장 (Best AUC: 0.7319)\n","Fold 1 | Epoch 21 | Train Loss: 0.0152 | Val AUC: 0.7300\n","Fold 1 | Epoch 22 | Train Loss: 0.0152 | Val AUC: 0.7281\n","Fold 1 | Epoch 23 | Train Loss: 0.0152 | Val AUC: 0.7277\n","Fold 1 | Epoch 24 | Train Loss: 0.0152 | Val AUC: 0.7319\n","Fold 1 | Epoch 25 | Train Loss: 0.0152 | Val AUC: 0.7326\n","✅ Fold 1 모델 저장 (Best AUC: 0.7326)\n","Fold 1 | Epoch 26 | Train Loss: 0.0152 | Val AUC: 0.7325\n","Fold 1 | Epoch 27 | Train Loss: 0.0152 | Val AUC: 0.7293\n","Fold 1 | Epoch 28 | Train Loss: 0.0152 | Val AUC: 0.7298\n","Fold 1 | Epoch 29 | Train Loss: 0.0152 | Val AUC: 0.7320\n","Fold 1 | Epoch 30 | Train Loss: 0.0152 | Val AUC: 0.7316\n","\n","=== Fold 2/5 학습 시작 ===\n","Fold 2 | Epoch 1 | Train Loss: 0.0187 | Val AUC: 0.6877\n","✅ Fold 2 모델 저장 (Best AUC: 0.6877)\n","Fold 2 | Epoch 2 | Train Loss: 0.0163 | Val AUC: 0.6984\n","✅ Fold 2 모델 저장 (Best AUC: 0.6984)\n","Fold 2 | Epoch 3 | Train Loss: 0.0159 | Val AUC: 0.7025\n","✅ Fold 2 모델 저장 (Best AUC: 0.7025)\n","Fold 2 | Epoch 4 | Train Loss: 0.0158 | Val AUC: 0.7076\n","✅ Fold 2 모델 저장 (Best AUC: 0.7076)\n","Fold 2 | Epoch 5 | Train Loss: 0.0157 | Val AUC: 0.7104\n","✅ Fold 2 모델 저장 (Best AUC: 0.7104)\n","Fold 2 | Epoch 6 | Train Loss: 0.0156 | Val AUC: 0.7142\n","✅ Fold 2 모델 저장 (Best AUC: 0.7142)\n","Fold 2 | Epoch 7 | Train Loss: 0.0156 | Val AUC: 0.7187\n","✅ Fold 2 모델 저장 (Best AUC: 0.7187)\n","Fold 2 | Epoch 8 | Train Loss: 0.0155 | Val AUC: 0.7163\n","Fold 2 | Epoch 9 | Train Loss: 0.0155 | Val AUC: 0.7213\n","✅ Fold 2 모델 저장 (Best AUC: 0.7213)\n","Fold 2 | Epoch 10 | Train Loss: 0.0155 | Val AUC: 0.7169\n","Fold 2 | Epoch 11 | Train Loss: 0.0154 | Val AUC: 0.7170\n","Fold 2 | Epoch 12 | Train Loss: 0.0154 | Val AUC: 0.7208\n","Fold 2 | Epoch 13 | Train Loss: 0.0154 | Val AUC: 0.7161\n","Fold 2 | Epoch 14 | Train Loss: 0.0154 | Val AUC: 0.7242\n","✅ Fold 2 모델 저장 (Best AUC: 0.7242)\n","Fold 2 | Epoch 15 | Train Loss: 0.0154 | Val AUC: 0.7230\n","Fold 2 | Epoch 16 | Train Loss: 0.0154 | Val AUC: 0.7253\n","✅ Fold 2 모델 저장 (Best AUC: 0.7253)\n","Fold 2 | Epoch 17 | Train Loss: 0.0154 | Val AUC: 0.7251\n","Fold 2 | Epoch 18 | Train Loss: 0.0153 | Val AUC: 0.7225\n","Fold 2 | Epoch 19 | Train Loss: 0.0153 | Val AUC: 0.7285\n","✅ Fold 2 모델 저장 (Best AUC: 0.7285)\n","Fold 2 | Epoch 20 | Train Loss: 0.0153 | Val AUC: 0.7266\n","Fold 2 | Epoch 21 | Train Loss: 0.0153 | Val AUC: 0.7267\n","Fold 2 | Epoch 22 | Train Loss: 0.0153 | Val AUC: 0.7268\n","Fold 2 | Epoch 23 | Train Loss: 0.0153 | Val AUC: 0.7269\n","Fold 2 | Epoch 24 | Train Loss: 0.0153 | Val AUC: 0.7272\n","Fold 2 | Epoch 25 | Train Loss: 0.0153 | Val AUC: 0.7290\n","✅ Fold 2 모델 저장 (Best AUC: 0.7290)\n","Fold 2 | Epoch 26 | Train Loss: 0.0152 | Val AUC: 0.7279\n","Fold 2 | Epoch 27 | Train Loss: 0.0152 | Val AUC: 0.7300\n","✅ Fold 2 모델 저장 (Best AUC: 0.7300)\n","Fold 2 | Epoch 28 | Train Loss: 0.0152 | Val AUC: 0.7278\n","Fold 2 | Epoch 29 | Train Loss: 0.0152 | Val AUC: 0.7244\n","Fold 2 | Epoch 30 | Train Loss: 0.0152 | Val AUC: 0.7298\n","\n","=== Fold 3/5 학습 시작 ===\n","Fold 3 | Epoch 1 | Train Loss: 0.0188 | Val AUC: 0.6964\n","✅ Fold 3 모델 저장 (Best AUC: 0.6964)\n","Fold 3 | Epoch 2 | Train Loss: 0.0164 | Val AUC: 0.7084\n","✅ Fold 3 모델 저장 (Best AUC: 0.7084)\n","Fold 3 | Epoch 3 | Train Loss: 0.0160 | Val AUC: 0.7081\n","Fold 3 | Epoch 4 | Train Loss: 0.0158 | Val AUC: 0.7156\n","✅ Fold 3 모델 저장 (Best AUC: 0.7156)\n","Fold 3 | Epoch 5 | Train Loss: 0.0158 | Val AUC: 0.7202\n","✅ Fold 3 모델 저장 (Best AUC: 0.7202)\n","Fold 3 | Epoch 6 | Train Loss: 0.0157 | Val AUC: 0.7205\n","✅ Fold 3 모델 저장 (Best AUC: 0.7205)\n","Fold 3 | Epoch 7 | Train Loss: 0.0156 | Val AUC: 0.7223\n","✅ Fold 3 모델 저장 (Best AUC: 0.7223)\n","Fold 3 | Epoch 8 | Train Loss: 0.0156 | Val AUC: 0.7261\n","✅ Fold 3 모델 저장 (Best AUC: 0.7261)\n","Fold 3 | Epoch 9 | Train Loss: 0.0156 | Val AUC: 0.7241\n","Fold 3 | Epoch 10 | Train Loss: 0.0155 | Val AUC: 0.7305\n","✅ Fold 3 모델 저장 (Best AUC: 0.7305)\n","Fold 3 | Epoch 11 | Train Loss: 0.0155 | Val AUC: 0.7296\n","Fold 3 | Epoch 12 | Train Loss: 0.0155 | Val AUC: 0.7276\n","Fold 3 | Epoch 13 | Train Loss: 0.0155 | Val AUC: 0.7317\n","✅ Fold 3 모델 저장 (Best AUC: 0.7317)\n","Fold 3 | Epoch 14 | Train Loss: 0.0155 | Val AUC: 0.7309\n","Fold 3 | Epoch 15 | Train Loss: 0.0154 | Val AUC: 0.7307\n","Fold 3 | Epoch 16 | Train Loss: 0.0154 | Val AUC: 0.7296\n","Fold 3 | Epoch 17 | Train Loss: 0.0154 | Val AUC: 0.7343\n","✅ Fold 3 모델 저장 (Best AUC: 0.7343)\n","Fold 3 | Epoch 18 | Train Loss: 0.0154 | Val AUC: 0.7358\n","✅ Fold 3 모델 저장 (Best AUC: 0.7358)\n","Fold 3 | Epoch 19 | Train Loss: 0.0154 | Val AUC: 0.7288\n","Fold 3 | Epoch 20 | Train Loss: 0.0154 | Val AUC: 0.7353\n","Fold 3 | Epoch 21 | Train Loss: 0.0154 | Val AUC: 0.7353\n","Fold 3 | Epoch 22 | Train Loss: 0.0154 | Val AUC: 0.7355\n","Fold 3 | Epoch 23 | Train Loss: 0.0153 | Val AUC: 0.7329\n","Fold 3 | Epoch 24 | Train Loss: 0.0153 | Val AUC: 0.7333\n","Fold 3 | Epoch 25 | Train Loss: 0.0153 | Val AUC: 0.7289\n","❌ Early Stopping (patience 7 도달)\n","\n","=== Fold 4/5 학습 시작 ===\n","Fold 4 | Epoch 1 | Train Loss: 0.0188 | Val AUC: 0.6945\n","✅ Fold 4 모델 저장 (Best AUC: 0.6945)\n","Fold 4 | Epoch 2 | Train Loss: 0.0163 | Val AUC: 0.6999\n","✅ Fold 4 모델 저장 (Best AUC: 0.6999)\n","Fold 4 | Epoch 3 | Train Loss: 0.0159 | Val AUC: 0.7156\n","✅ Fold 4 모델 저장 (Best AUC: 0.7156)\n","Fold 4 | Epoch 4 | Train Loss: 0.0157 | Val AUC: 0.7145\n","Fold 4 | Epoch 5 | Train Loss: 0.0156 | Val AUC: 0.7210\n","✅ Fold 4 모델 저장 (Best AUC: 0.7210)\n","Fold 4 | Epoch 6 | Train Loss: 0.0156 | Val AUC: 0.7222\n","✅ Fold 4 모델 저장 (Best AUC: 0.7222)\n","Fold 4 | Epoch 7 | Train Loss: 0.0155 | Val AUC: 0.7274\n","✅ Fold 4 모델 저장 (Best AUC: 0.7274)\n","Fold 4 | Epoch 8 | Train Loss: 0.0155 | Val AUC: 0.7250\n","Fold 4 | Epoch 9 | Train Loss: 0.0155 | Val AUC: 0.7300\n","✅ Fold 4 모델 저장 (Best AUC: 0.7300)\n","Fold 4 | Epoch 10 | Train Loss: 0.0154 | Val AUC: 0.7255\n","Fold 4 | Epoch 11 | Train Loss: 0.0154 | Val AUC: 0.7257\n","Fold 4 | Epoch 12 | Train Loss: 0.0154 | Val AUC: 0.7293\n","Fold 4 | Epoch 13 | Train Loss: 0.0154 | Val AUC: 0.7307\n","✅ Fold 4 모델 저장 (Best AUC: 0.7307)\n","Fold 4 | Epoch 14 | Train Loss: 0.0153 | Val AUC: 0.7240\n","Fold 4 | Epoch 15 | Train Loss: 0.0153 | Val AUC: 0.7313\n","✅ Fold 4 모델 저장 (Best AUC: 0.7313)\n","Fold 4 | Epoch 16 | Train Loss: 0.0153 | Val AUC: 0.7336\n","✅ Fold 4 모델 저장 (Best AUC: 0.7336)\n","Fold 4 | Epoch 17 | Train Loss: 0.0153 | Val AUC: 0.7337\n","✅ Fold 4 모델 저장 (Best AUC: 0.7337)\n","Fold 4 | Epoch 18 | Train Loss: 0.0153 | Val AUC: 0.7334\n","Fold 4 | Epoch 19 | Train Loss: 0.0153 | Val AUC: 0.7317\n","Fold 4 | Epoch 20 | Train Loss: 0.0152 | Val AUC: 0.7319\n","Fold 4 | Epoch 21 | Train Loss: 0.0152 | Val AUC: 0.7302\n","Fold 4 | Epoch 22 | Train Loss: 0.0152 | Val AUC: 0.7337\n","Fold 4 | Epoch 23 | Train Loss: 0.0152 | Val AUC: 0.7374\n","✅ Fold 4 모델 저장 (Best AUC: 0.7374)\n","Fold 4 | Epoch 24 | Train Loss: 0.0152 | Val AUC: 0.7329\n","Fold 4 | Epoch 25 | Train Loss: 0.0152 | Val AUC: 0.7305\n","Fold 4 | Epoch 26 | Train Loss: 0.0152 | Val AUC: 0.7322\n","Fold 4 | Epoch 27 | Train Loss: 0.0152 | Val AUC: 0.7279\n","Fold 4 | Epoch 28 | Train Loss: 0.0152 | Val AUC: 0.7275\n","Fold 4 | Epoch 29 | Train Loss: 0.0152 | Val AUC: 0.7346\n","Fold 4 | Epoch 30 | Train Loss: 0.0152 | Val AUC: 0.7289\n","❌ Early Stopping (patience 7 도달)\n","\n","=== Fold 5/5 학습 시작 ===\n","Fold 5 | Epoch 1 | Train Loss: 0.0186 | Val AUC: 0.6964\n","✅ Fold 5 모델 저장 (Best AUC: 0.6964)\n","Fold 5 | Epoch 2 | Train Loss: 0.0162 | Val AUC: 0.7081\n","✅ Fold 5 모델 저장 (Best AUC: 0.7081)\n","Fold 5 | Epoch 3 | Train Loss: 0.0158 | Val AUC: 0.7130\n","✅ Fold 5 모델 저장 (Best AUC: 0.7130)\n","Fold 5 | Epoch 4 | Train Loss: 0.0157 | Val AUC: 0.7162\n","✅ Fold 5 모델 저장 (Best AUC: 0.7162)\n","Fold 5 | Epoch 5 | Train Loss: 0.0156 | Val AUC: 0.7188\n","✅ Fold 5 모델 저장 (Best AUC: 0.7188)\n","Fold 5 | Epoch 6 | Train Loss: 0.0155 | Val AUC: 0.7220\n","✅ Fold 5 모델 저장 (Best AUC: 0.7220)\n","Fold 5 | Epoch 7 | Train Loss: 0.0155 | Val AUC: 0.7261\n","✅ Fold 5 모델 저장 (Best AUC: 0.7261)\n","Fold 5 | Epoch 8 | Train Loss: 0.0154 | Val AUC: 0.7227\n","Fold 5 | Epoch 9 | Train Loss: 0.0154 | Val AUC: 0.7282\n","✅ Fold 5 모델 저장 (Best AUC: 0.7282)\n","Fold 5 | Epoch 10 | Train Loss: 0.0154 | Val AUC: 0.7278\n","Fold 5 | Epoch 11 | Train Loss: 0.0153 | Val AUC: 0.7235\n","Fold 5 | Epoch 12 | Train Loss: 0.0153 | Val AUC: 0.7289\n","✅ Fold 5 모델 저장 (Best AUC: 0.7289)\n","Fold 5 | Epoch 13 | Train Loss: 0.0153 | Val AUC: 0.7282\n","Fold 5 | Epoch 14 | Train Loss: 0.0153 | Val AUC: 0.7281\n","Fold 5 | Epoch 15 | Train Loss: 0.0153 | Val AUC: 0.7330\n","✅ Fold 5 모델 저장 (Best AUC: 0.7330)\n","Fold 5 | Epoch 16 | Train Loss: 0.0152 | Val AUC: 0.7319\n","Fold 5 | Epoch 17 | Train Loss: 0.0152 | Val AUC: 0.7322\n","Fold 5 | Epoch 18 | Train Loss: 0.0152 | Val AUC: 0.7331\n","✅ Fold 5 모델 저장 (Best AUC: 0.7331)\n","Fold 5 | Epoch 19 | Train Loss: 0.0152 | Val AUC: 0.7304\n","Fold 5 | Epoch 20 | Train Loss: 0.0152 | Val AUC: 0.7317\n","Fold 5 | Epoch 21 | Train Loss: 0.0152 | Val AUC: 0.7362\n","✅ Fold 5 모델 저장 (Best AUC: 0.7362)\n","Fold 5 | Epoch 22 | Train Loss: 0.0152 | Val AUC: 0.7298\n","Fold 5 | Epoch 23 | Train Loss: 0.0151 | Val AUC: 0.7311\n","Fold 5 | Epoch 24 | Train Loss: 0.0151 | Val AUC: 0.7350\n","Fold 5 | Epoch 25 | Train Loss: 0.0151 | Val AUC: 0.7307\n","Fold 5 | Epoch 26 | Train Loss: 0.0151 | Val AUC: 0.7311\n","Fold 5 | Epoch 27 | Train Loss: 0.0151 | Val AUC: 0.7326\n","Fold 5 | Epoch 28 | Train Loss: 0.0151 | Val AUC: 0.7353\n","❌ Early Stopping (patience 7 도달)\n"]}]},{"cell_type":"code","source":["# 6. 전체 OOF AUC\n","final_auc = roc_auc_score(y_train, oof_preds)\n","print(f\"\\n전체 OOF AUC: {final_auc:.4f}\")\n","\n","# 6-1. 전체 OOF 결과를 로그에 추가\n","log_df = pd.DataFrame(all_logs)\n","log_df = pd.concat([\n","    log_df,\n","    pd.DataFrame([{\n","        'fold': 0,\n","        'epoch': 0,\n","        'train_loss': None,\n","        'val_auc': final_auc\n","    }])\n","], ignore_index=True)\n","\n","# 7. 최종 로그 CSV 저장\n","log_df.to_csv(BASE_DIR + 'logs/deep_mlp_training_log_1.csv', index=False, encoding='utf-8')\n","print(f\"전체 학습 로그 저장 완료: {BASE_DIR}logs/deep_mlp_training_log_1.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hiUuVdbLrLiN","executionInfo":{"status":"ok","timestamp":1751557381157,"user_tz":-540,"elapsed":348,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"569b9022-633f-4007-a6b8-9bb09bd50c89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ 전체 OOF AUC: 0.7298\n","✅ 전체 학습 로그 저장 완료: /content/drive/MyDrive/Dacon_FakeText/logs/deep_mlp_training_log_1.csv\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-42-2646260763.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  log_df = pd.concat([\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pIELUIZj11Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EZsFGhDYFIxW"},"execution_count":null,"outputs":[]}]}