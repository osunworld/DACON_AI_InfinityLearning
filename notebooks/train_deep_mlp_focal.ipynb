{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"91bZ1iB6qffl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751563377690,"user_tz":-540,"elapsed":21892,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"dcf0a08a-c29a-4e25-9bb5-16223617b4ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","\n","\n","# 베이스 경로 설정\n","BASE_DIR = '/content/drive/MyDrive/Dacon_FakeText/'\n","SAVE_PATH = BASE_DIR + 'data/embeddings/train_concat.npy'"]},{"cell_type":"code","source":["!pip install iterative-stratification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDOVdsf1Y5k2","executionInfo":{"status":"ok","timestamp":1751563384051,"user_tz":-540,"elapsed":6357,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"76f120b7-7dcc-4f63-fad3-83ffd0b01ede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting iterative-stratification\n","  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.15.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.6.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n","Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.9\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from tqdm import tqdm\n","import os"],"metadata":{"id":"SqkF_FooqqsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ✅ 1. 설정값\n","n_splits = 5\n","batch_size = 512\n","epochs = 30\n","early_stopping_rounds = 4\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"bDdnuWkqqzda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ✅ 2. 데이터 로딩\n","X_train = np.load(BASE_DIR + 'data/embeddings/train_concat.npy')\n","train_meta = pd.read_csv(BASE_DIR + 'data/train_paragraph.csv')\n","y_train = train_meta['generated'].values\n","groups = train_meta['title'].values\n","\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cCQGJr_q58B","executionInfo":{"status":"ok","timestamp":1751563549488,"user_tz":-540,"elapsed":160811,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"587462ba-5bb1-410f-e843-38c75db9acd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (1226364, 775), y_train shape: (1226364,)\n"]}]},{"cell_type":"code","source":["# ✅ 3. PyTorch Dataset 클래스\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.y = torch.tensor(y, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n"],"metadata":{"id":"qN7WlfmKrLoN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.eps = 1e-8  # log 안정성 확보용\n","\n","    def forward(self, inputs, targets):\n","        # inputs: sigmoid 통과된 예측 (batch_size, 1)\n","        # targets: (batch_size, 1)\n","        BCE = -(targets * torch.log(inputs + self.eps) + (1 - targets) * torch.log(1 - inputs + self.eps))\n","        focal = self.alpha * (1 - inputs)**self.gamma * targets * BCE + \\\n","                (1 - self.alpha) * inputs**self.gamma * (1 - targets) * BCE\n","        return focal.mean()"],"metadata":{"id":"QjhjYLn9Ay9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class StratifiedGroupKFold:\n","    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n","        self.n_splits = n_splits\n","        self.shuffle = shuffle\n","        self.random_state = random_state\n","\n","    def split(self, X, y, groups):\n","        # 그룹을 정수로 인코딩\n","        if not np.issubdtype(groups.dtype, np.number):\n","            groups = LabelEncoder().fit_transform(groups)\n","\n","        # 그룹별 인덱스 저장\n","        group_to_indices = {}\n","        for idx, g in enumerate(groups):\n","            group_to_indices.setdefault(g, []).append(idx)\n","\n","        unique_groups = np.array(list(group_to_indices.keys()))\n","        group_y = np.array([\n","            int(y[group_to_indices[g]].mean() >= 0.5) for g in unique_groups\n","        ])\n","\n","        skf = StratifiedKFold(\n","            n_splits=self.n_splits,\n","            shuffle=self.shuffle,\n","            random_state=self.random_state\n","        )\n","\n","        for group_train_idx, group_val_idx in skf.split(unique_groups, group_y):\n","            train_indices, val_indices = [], []\n","\n","            for gi in group_train_idx:\n","                train_indices.extend(group_to_indices[unique_groups[gi]])\n","            for gi in group_val_idx:\n","                val_indices.extend(group_to_indices[unique_groups[gi]])\n","\n","            yield np.array(train_indices), np.array(val_indices)"],"metadata":{"id":"JlkEQMEvSOAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. MLP 모델 클래스\n","class MLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(MLP, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(64,1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"PxbUe4CCspqb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ✅ 5. GroupKFold + 학습 + 로그 저장\n","sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n","oof_preds = np.zeros(len(X_train))\n","\n","all_logs = []  # 전체 로그 저장용 리스트\n","y_train=y_train.astype(np.float32)\n","for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train, y_train, groups)):\n","    print(f'\\n=== Fold {fold+1}/{n_splits} 학습 시작 ===')\n","\n","    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n","    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n","\n","    train_dataset = CustomDataset(X_tr, y_tr)\n","    val_dataset = CustomDataset(X_val, y_val)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    model = MLP(input_dim=X_train.shape[1]).to(device)\n","    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n","\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='max',           # val_auc가 증가해야 하므로 'max'\n","    factor=0.5,           # lr을 절반으로 줄임\n","    patience=3,           # 2 epoch 동안 개선 없으면 감소\n","    verbose=True,\n","    min_lr=1e-5           # 최소 학습률 하한\n","    )\n","\n","    best_auc = 0\n","    patience = 0\n","    fold_log = []  # 이 fold의 epoch별 로그 저장용 리스트\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            optimizer.zero_grad()\n","            preds = model(xb)\n","            loss = criterion(preds, yb.view(-1,1))\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Validation\n","        model.eval()\n","        val_preds = []\n","        with torch.no_grad():\n","            for xb, _ in val_loader:\n","                xb = xb.to(device)\n","                pred = model(xb).squeeze()\n","                pred = torch.sigmoid(pred)\n","                val_preds.extend(pred.cpu().numpy())\n","\n","        auc = roc_auc_score(y_val, val_preds)\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        print(f\"Fold {fold+1} | Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val AUC: {auc:.4f}\")\n","\n","        # LR Scheduler 업데이트\n","        scheduler.step(auc)\n","\n","        # ✅ 로그 저장\n","        fold_log.append({\n","            'fold': fold + 1,\n","            'epoch': epoch + 1,\n","            'train_loss': avg_train_loss,\n","            'val_auc': auc\n","        })\n","\n","        # Early Stopping\n","        if auc > best_auc:\n","            best_auc = auc\n","            patience = 0\n","            torch.save(model.state_dict(), BASE_DIR + f'model/mlp_fold{fold+1}.pt')\n","            print(f\"Fold {fold+1} 모델 저장 (Best AUC: {best_auc:.4f})\")\n","        else:\n","            patience += 1\n","            if patience >= early_stopping_rounds:\n","                print(f\"Early Stopping (patience {early_stopping_rounds} 도달)\")\n","                break\n","\n","    # Fold별 로그 저장\n","    all_logs.extend(fold_log)\n","\n","    # Fold OOF\n","    oof_preds[val_idx] = val_preds\n","\n"],"metadata":{"id":"A30Y75EXrLlE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751566451019,"user_tz":-540,"elapsed":2113384,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"d213bcff-38ba-4818-a842-14eb49fc7701"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Fold 1/5 학습 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 1 | Epoch 1 | Train Loss: 0.0273 | Val AUC: 0.6921\n","✅ Fold 1 모델 저장 (Best AUC: 0.6921)\n","Fold 1 | Epoch 2 | Train Loss: 0.0231 | Val AUC: 0.7047\n","✅ Fold 1 모델 저장 (Best AUC: 0.7047)\n","Fold 1 | Epoch 3 | Train Loss: 0.0223 | Val AUC: 0.7103\n","✅ Fold 1 모델 저장 (Best AUC: 0.7103)\n","Fold 1 | Epoch 4 | Train Loss: 0.0220 | Val AUC: 0.7159\n","✅ Fold 1 모델 저장 (Best AUC: 0.7159)\n","Fold 1 | Epoch 5 | Train Loss: 0.0218 | Val AUC: 0.7175\n","✅ Fold 1 모델 저장 (Best AUC: 0.7175)\n","Fold 1 | Epoch 6 | Train Loss: 0.0217 | Val AUC: 0.7181\n","✅ Fold 1 모델 저장 (Best AUC: 0.7181)\n","Fold 1 | Epoch 7 | Train Loss: 0.0217 | Val AUC: 0.7194\n","✅ Fold 1 모델 저장 (Best AUC: 0.7194)\n","Fold 1 | Epoch 8 | Train Loss: 0.0216 | Val AUC: 0.7216\n","✅ Fold 1 모델 저장 (Best AUC: 0.7216)\n","Fold 1 | Epoch 9 | Train Loss: 0.0215 | Val AUC: 0.7221\n","✅ Fold 1 모델 저장 (Best AUC: 0.7221)\n","Fold 1 | Epoch 10 | Train Loss: 0.0215 | Val AUC: 0.7223\n","✅ Fold 1 모델 저장 (Best AUC: 0.7223)\n","Fold 1 | Epoch 11 | Train Loss: 0.0214 | Val AUC: 0.7248\n","✅ Fold 1 모델 저장 (Best AUC: 0.7248)\n","Fold 1 | Epoch 12 | Train Loss: 0.0214 | Val AUC: 0.7232\n","Fold 1 | Epoch 13 | Train Loss: 0.0214 | Val AUC: 0.7253\n","✅ Fold 1 모델 저장 (Best AUC: 0.7253)\n","Fold 1 | Epoch 14 | Train Loss: 0.0213 | Val AUC: 0.7276\n","✅ Fold 1 모델 저장 (Best AUC: 0.7276)\n","Fold 1 | Epoch 15 | Train Loss: 0.0213 | Val AUC: 0.7265\n","Fold 1 | Epoch 16 | Train Loss: 0.0212 | Val AUC: 0.7269\n","Fold 1 | Epoch 17 | Train Loss: 0.0212 | Val AUC: 0.7288\n","✅ Fold 1 모델 저장 (Best AUC: 0.7288)\n","Fold 1 | Epoch 18 | Train Loss: 0.0212 | Val AUC: 0.7276\n","Fold 1 | Epoch 19 | Train Loss: 0.0212 | Val AUC: 0.7273\n","Fold 1 | Epoch 20 | Train Loss: 0.0212 | Val AUC: 0.7242\n","Fold 1 | Epoch 21 | Train Loss: 0.0211 | Val AUC: 0.7291\n","✅ Fold 1 모델 저장 (Best AUC: 0.7291)\n","Fold 1 | Epoch 22 | Train Loss: 0.0211 | Val AUC: 0.7299\n","✅ Fold 1 모델 저장 (Best AUC: 0.7299)\n","Fold 1 | Epoch 23 | Train Loss: 0.0211 | Val AUC: 0.7315\n","✅ Fold 1 모델 저장 (Best AUC: 0.7315)\n","Fold 1 | Epoch 24 | Train Loss: 0.0211 | Val AUC: 0.7297\n","Fold 1 | Epoch 25 | Train Loss: 0.0211 | Val AUC: 0.7310\n","Fold 1 | Epoch 26 | Train Loss: 0.0210 | Val AUC: 0.7309\n","Fold 1 | Epoch 27 | Train Loss: 0.0210 | Val AUC: 0.7293\n","❌ Early Stopping (patience 4 도달)\n","\n","=== Fold 2/5 학습 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 2 | Epoch 1 | Train Loss: 0.0277 | Val AUC: 0.6877\n","✅ Fold 2 모델 저장 (Best AUC: 0.6877)\n","Fold 2 | Epoch 2 | Train Loss: 0.0234 | Val AUC: 0.7002\n","✅ Fold 2 모델 저장 (Best AUC: 0.7002)\n","Fold 2 | Epoch 3 | Train Loss: 0.0225 | Val AUC: 0.7076\n","✅ Fold 2 모델 저장 (Best AUC: 0.7076)\n","Fold 2 | Epoch 4 | Train Loss: 0.0221 | Val AUC: 0.7106\n","✅ Fold 2 모델 저장 (Best AUC: 0.7106)\n","Fold 2 | Epoch 5 | Train Loss: 0.0220 | Val AUC: 0.7148\n","✅ Fold 2 모델 저장 (Best AUC: 0.7148)\n","Fold 2 | Epoch 6 | Train Loss: 0.0218 | Val AUC: 0.7171\n","✅ Fold 2 모델 저장 (Best AUC: 0.7171)\n","Fold 2 | Epoch 7 | Train Loss: 0.0217 | Val AUC: 0.7184\n","✅ Fold 2 모델 저장 (Best AUC: 0.7184)\n","Fold 2 | Epoch 8 | Train Loss: 0.0217 | Val AUC: 0.7196\n","✅ Fold 2 모델 저장 (Best AUC: 0.7196)\n","Fold 2 | Epoch 9 | Train Loss: 0.0216 | Val AUC: 0.7217\n","✅ Fold 2 모델 저장 (Best AUC: 0.7217)\n","Fold 2 | Epoch 10 | Train Loss: 0.0216 | Val AUC: 0.7183\n","Fold 2 | Epoch 11 | Train Loss: 0.0215 | Val AUC: 0.7223\n","✅ Fold 2 모델 저장 (Best AUC: 0.7223)\n","Fold 2 | Epoch 12 | Train Loss: 0.0215 | Val AUC: 0.7208\n","Fold 2 | Epoch 13 | Train Loss: 0.0215 | Val AUC: 0.7215\n","Fold 2 | Epoch 14 | Train Loss: 0.0214 | Val AUC: 0.7232\n","✅ Fold 2 모델 저장 (Best AUC: 0.7232)\n","Fold 2 | Epoch 15 | Train Loss: 0.0214 | Val AUC: 0.7217\n","Fold 2 | Epoch 16 | Train Loss: 0.0213 | Val AUC: 0.7240\n","✅ Fold 2 모델 저장 (Best AUC: 0.7240)\n","Fold 2 | Epoch 17 | Train Loss: 0.0213 | Val AUC: 0.7209\n","Fold 2 | Epoch 18 | Train Loss: 0.0213 | Val AUC: 0.7231\n","Fold 2 | Epoch 19 | Train Loss: 0.0213 | Val AUC: 0.7240\n","✅ Fold 2 모델 저장 (Best AUC: 0.7240)\n","Fold 2 | Epoch 20 | Train Loss: 0.0213 | Val AUC: 0.7230\n","Fold 2 | Epoch 21 | Train Loss: 0.0209 | Val AUC: 0.7287\n","✅ Fold 2 모델 저장 (Best AUC: 0.7287)\n","Fold 2 | Epoch 22 | Train Loss: 0.0209 | Val AUC: 0.7277\n","Fold 2 | Epoch 23 | Train Loss: 0.0208 | Val AUC: 0.7307\n","✅ Fold 2 모델 저장 (Best AUC: 0.7307)\n","Fold 2 | Epoch 24 | Train Loss: 0.0208 | Val AUC: 0.7287\n","Fold 2 | Epoch 25 | Train Loss: 0.0207 | Val AUC: 0.7328\n","✅ Fold 2 모델 저장 (Best AUC: 0.7328)\n","Fold 2 | Epoch 26 | Train Loss: 0.0207 | Val AUC: 0.7313\n","Fold 2 | Epoch 27 | Train Loss: 0.0207 | Val AUC: 0.7308\n","Fold 2 | Epoch 28 | Train Loss: 0.0206 | Val AUC: 0.7331\n","✅ Fold 2 모델 저장 (Best AUC: 0.7331)\n","Fold 2 | Epoch 29 | Train Loss: 0.0206 | Val AUC: 0.7300\n","Fold 2 | Epoch 30 | Train Loss: 0.0206 | Val AUC: 0.7313\n","\n","=== Fold 3/5 학습 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 3 | Epoch 1 | Train Loss: 0.0277 | Val AUC: 0.6963\n","✅ Fold 3 모델 저장 (Best AUC: 0.6963)\n","Fold 3 | Epoch 2 | Train Loss: 0.0234 | Val AUC: 0.7118\n","✅ Fold 3 모델 저장 (Best AUC: 0.7118)\n","Fold 3 | Epoch 3 | Train Loss: 0.0224 | Val AUC: 0.7156\n","✅ Fold 3 모델 저장 (Best AUC: 0.7156)\n","Fold 3 | Epoch 4 | Train Loss: 0.0222 | Val AUC: 0.7188\n","✅ Fold 3 모델 저장 (Best AUC: 0.7188)\n","Fold 3 | Epoch 5 | Train Loss: 0.0220 | Val AUC: 0.7235\n","✅ Fold 3 모델 저장 (Best AUC: 0.7235)\n","Fold 3 | Epoch 6 | Train Loss: 0.0219 | Val AUC: 0.7252\n","✅ Fold 3 모델 저장 (Best AUC: 0.7252)\n","Fold 3 | Epoch 7 | Train Loss: 0.0218 | Val AUC: 0.7253\n","✅ Fold 3 모델 저장 (Best AUC: 0.7253)\n","Fold 3 | Epoch 8 | Train Loss: 0.0217 | Val AUC: 0.7266\n","✅ Fold 3 모델 저장 (Best AUC: 0.7266)\n","Fold 3 | Epoch 9 | Train Loss: 0.0217 | Val AUC: 0.7301\n","✅ Fold 3 모델 저장 (Best AUC: 0.7301)\n","Fold 3 | Epoch 10 | Train Loss: 0.0216 | Val AUC: 0.7266\n","Fold 3 | Epoch 11 | Train Loss: 0.0216 | Val AUC: 0.7332\n","✅ Fold 3 모델 저장 (Best AUC: 0.7332)\n","Fold 3 | Epoch 12 | Train Loss: 0.0216 | Val AUC: 0.7291\n","Fold 3 | Epoch 13 | Train Loss: 0.0215 | Val AUC: 0.7278\n","Fold 3 | Epoch 14 | Train Loss: 0.0215 | Val AUC: 0.7340\n","✅ Fold 3 모델 저장 (Best AUC: 0.7340)\n","Fold 3 | Epoch 15 | Train Loss: 0.0215 | Val AUC: 0.7336\n","Fold 3 | Epoch 16 | Train Loss: 0.0214 | Val AUC: 0.7328\n","Fold 3 | Epoch 17 | Train Loss: 0.0214 | Val AUC: 0.7335\n","Fold 3 | Epoch 18 | Train Loss: 0.0214 | Val AUC: 0.7352\n","✅ Fold 3 모델 저장 (Best AUC: 0.7352)\n","Fold 3 | Epoch 19 | Train Loss: 0.0214 | Val AUC: 0.7330\n","Fold 3 | Epoch 20 | Train Loss: 0.0214 | Val AUC: 0.7325\n","Fold 3 | Epoch 21 | Train Loss: 0.0213 | Val AUC: 0.7364\n","✅ Fold 3 모델 저장 (Best AUC: 0.7364)\n","Fold 3 | Epoch 22 | Train Loss: 0.0213 | Val AUC: 0.7326\n","Fold 3 | Epoch 23 | Train Loss: 0.0213 | Val AUC: 0.7331\n","Fold 3 | Epoch 24 | Train Loss: 0.0213 | Val AUC: 0.7338\n","Fold 3 | Epoch 25 | Train Loss: 0.0213 | Val AUC: 0.7353\n","❌ Early Stopping (patience 4 도달)\n","\n","=== Fold 4/5 학습 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 4 | Epoch 1 | Train Loss: 0.0272 | Val AUC: 0.6991\n","✅ Fold 4 모델 저장 (Best AUC: 0.6991)\n","Fold 4 | Epoch 2 | Train Loss: 0.0231 | Val AUC: 0.7020\n","✅ Fold 4 모델 저장 (Best AUC: 0.7020)\n","Fold 4 | Epoch 3 | Train Loss: 0.0223 | Val AUC: 0.7128\n","✅ Fold 4 모델 저장 (Best AUC: 0.7128)\n","Fold 4 | Epoch 4 | Train Loss: 0.0220 | Val AUC: 0.7169\n","✅ Fold 4 모델 저장 (Best AUC: 0.7169)\n","Fold 4 | Epoch 5 | Train Loss: 0.0219 | Val AUC: 0.7204\n","✅ Fold 4 모델 저장 (Best AUC: 0.7204)\n","Fold 4 | Epoch 6 | Train Loss: 0.0217 | Val AUC: 0.7185\n","Fold 4 | Epoch 7 | Train Loss: 0.0216 | Val AUC: 0.7237\n","✅ Fold 4 모델 저장 (Best AUC: 0.7237)\n","Fold 4 | Epoch 8 | Train Loss: 0.0216 | Val AUC: 0.7207\n","Fold 4 | Epoch 9 | Train Loss: 0.0215 | Val AUC: 0.7299\n","✅ Fold 4 모델 저장 (Best AUC: 0.7299)\n","Fold 4 | Epoch 10 | Train Loss: 0.0214 | Val AUC: 0.7317\n","✅ Fold 4 모델 저장 (Best AUC: 0.7317)\n","Fold 4 | Epoch 11 | Train Loss: 0.0214 | Val AUC: 0.7242\n","Fold 4 | Epoch 12 | Train Loss: 0.0214 | Val AUC: 0.7268\n","Fold 4 | Epoch 13 | Train Loss: 0.0213 | Val AUC: 0.7238\n","Fold 4 | Epoch 14 | Train Loss: 0.0213 | Val AUC: 0.7248\n","❌ Early Stopping (patience 4 도달)\n","\n","=== Fold 5/5 학습 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 5 | Epoch 1 | Train Loss: 0.0269 | Val AUC: 0.6971\n","✅ Fold 5 모델 저장 (Best AUC: 0.6971)\n","Fold 5 | Epoch 2 | Train Loss: 0.0229 | Val AUC: 0.7117\n","✅ Fold 5 모델 저장 (Best AUC: 0.7117)\n","Fold 5 | Epoch 3 | Train Loss: 0.0221 | Val AUC: 0.7136\n","✅ Fold 5 모델 저장 (Best AUC: 0.7136)\n","Fold 5 | Epoch 4 | Train Loss: 0.0219 | Val AUC: 0.7218\n","✅ Fold 5 모델 저장 (Best AUC: 0.7218)\n","Fold 5 | Epoch 5 | Train Loss: 0.0217 | Val AUC: 0.7219\n","✅ Fold 5 모델 저장 (Best AUC: 0.7219)\n","Fold 5 | Epoch 6 | Train Loss: 0.0216 | Val AUC: 0.7262\n","✅ Fold 5 모델 저장 (Best AUC: 0.7262)\n","Fold 5 | Epoch 7 | Train Loss: 0.0215 | Val AUC: 0.7278\n","✅ Fold 5 모델 저장 (Best AUC: 0.7278)\n","Fold 5 | Epoch 8 | Train Loss: 0.0215 | Val AUC: 0.7266\n","Fold 5 | Epoch 9 | Train Loss: 0.0213 | Val AUC: 0.7264\n","Fold 5 | Epoch 10 | Train Loss: 0.0213 | Val AUC: 0.7287\n","✅ Fold 5 모델 저장 (Best AUC: 0.7287)\n","Fold 5 | Epoch 11 | Train Loss: 0.0213 | Val AUC: 0.7334\n","✅ Fold 5 모델 저장 (Best AUC: 0.7334)\n","Fold 5 | Epoch 12 | Train Loss: 0.0212 | Val AUC: 0.7312\n","Fold 5 | Epoch 13 | Train Loss: 0.0212 | Val AUC: 0.7312\n","Fold 5 | Epoch 14 | Train Loss: 0.0212 | Val AUC: 0.7305\n","Fold 5 | Epoch 15 | Train Loss: 0.0212 | Val AUC: 0.7273\n","❌ Early Stopping (patience 4 도달)\n"]}]},{"cell_type":"code","source":["# 6. 전체 OOF AUC\n","final_auc = roc_auc_score(y_train, oof_preds)\n","print(f\"\\n전체 OOF AUC: {final_auc:.4f}\")\n","\n","# 6-1. 전체 OOF 결과를 로그에 추가\n","log_df = pd.DataFrame(all_logs)\n","log_df = pd.concat([\n","    log_df,\n","    pd.DataFrame([{\n","        'fold': 0,\n","        'epoch': 0,\n","        'train_loss': None,\n","        'val_auc': final_auc\n","    }])\n","], ignore_index=True)\n","\n","# 7. 최종 로그 CSV 저장\n","log_df.to_csv(BASE_DIR + 'logs/deep_mlp_training_log_3.csv', index=False, encoding='utf-8')\n","print(f\"전체 학습 로그 저장 완료: {BASE_DIR}logs/deep_mlp_training_log_3.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hiUuVdbLrLiN","executionInfo":{"status":"ok","timestamp":1751566474671,"user_tz":-540,"elapsed":832,"user":{"displayName":"정태양","userId":"07065479975462546638"}},"outputId":"c8e485b7-4503-4fd5-d7ee-3a0f7d9ca8d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ 전체 OOF AUC: 0.7263\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-15-2918466026.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  log_df = pd.concat([\n"]},{"output_type":"stream","name":"stdout","text":["✅ 전체 학습 로그 저장 완료: /content/drive/MyDrive/Dacon_FakeText/logs/deep_mlp_training_log_3.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pIELUIZj11Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EZsFGhDYFIxW"},"execution_count":null,"outputs":[]}]}